{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cfeb0594",
   "metadata": {},
   "source": [
    "Simple sampling with pandas\n",
    "Throughout this chapter, you'll be exploring song data from Spotify. Each row of this population dataset represents a song, and there are over 40,000 rows. Columns include the song name, the artists who performed it, the release year, and attributes of the song like its duration, tempo, and danceability. You'll start by looking at the durations.\n",
    "\n",
    "Your first task is to sample the Spotify dataset and compare the mean duration of the population with the sample.\n",
    "\n",
    "spotify_population is available and pandas is loaded as pd.\n",
    "\n",
    "Instructions\n",
    "\n",
    "Sample 1000 rows from spotify_population, assigning to spotify_sample.\n",
    "Calculate the mean duration in minutes from spotify_population using pandas.\n",
    "Calculate the mean duration in minutes from spotify_sample using pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0995c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample 1000 rows from spotify_population\n",
    "spotify_sample = spotify_population.sample(n=1000)\n",
    "\n",
    "# Print the sample\n",
    "print(spotify_sample)\n",
    "\n",
    "# Calculate the mean duration in mins from spotify_population\n",
    "mean_dur_pop = spotify_population['duration_minutes'].mean()\n",
    "\n",
    "# Calculate the mean duration in mins from spotify_sample\n",
    "mean_dur_samp = spotify_sample['duration_minutes'].mean()\n",
    "\n",
    "# Print the means\n",
    "print(mean_dur_pop)\n",
    "print(mean_dur_samp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd2d970",
   "metadata": {},
   "source": [
    "Simple sampling and calculating with NumPy\n",
    "You can also use numpy to calculate parameters or statistics from a list or pandas Series.\n",
    "\n",
    "You'll be turning it up to eleven and looking at the loudness property of each song.\n",
    "\n",
    "spotify_population is available and numpy is loaded as np.\n",
    "\n",
    "Instructions\n",
    "\n",
    "Subset the loudness column from spotify_population to create the pandas Series, loudness_pop.\n",
    "Sample loudness_pop to get 100 random values, assigning to loudness_samp.\n",
    "Calculate the mean of loudness_pop using numpy.\n",
    "Calculate the mean of loudness_samp using numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4c296d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pandas Series from the loudness column of spotify_population\n",
    "loudness_pop = spotify_population['loudness']\n",
    "\n",
    "# Sample 100 values of loudness_pop\n",
    "loudness_samp = loudness_pop.sample(n=100)\n",
    "\n",
    "# Calculate the mean of loudness_pop\n",
    "mean_loudness_pop = loudness_pop.mean()\n",
    "\n",
    "# Calculate the mean of loudness_samp\n",
    "mean_loudness_samp = loudness_samp.mean()\n",
    "\n",
    "# Print the means\n",
    "print(mean_loudness_pop)\n",
    "print(mean_loudness_samp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc3d53e",
   "metadata": {},
   "source": [
    "Are the findings from this sample generalizable?\n",
    "You just saw how convenience sampling—collecting data using the easiest method—can result in samples that aren't representative of the population. Equivalently, this means findings from the sample are not generalizable to the population. Visualizing the distributions of the population and the sample can help determine whether or not the sample is representative of the population.\n",
    "\n",
    "The Spotify dataset contains an acousticness column, which is a confidence measure from zero to one of whether the track was made with instruments that aren't plugged in. You'll compare the acousticness distribution of the total population of songs with a sample of those songs.\n",
    "\n",
    "spotify_population and spotify_mysterious_sample are available; pandas as pd, matplotlib.pyplot as plt, and numpy as np are loaded.\n",
    "\n",
    "Instructions\n",
    "\n",
    "Plot a histogram of the acousticness from spotify_population with bins of width 0.01 from 0 to 1 using pandas .hist().\n",
    "Update the histogram code to use the spotify_mysterious_sample dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54e35a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the distribution of acousticness with a histogram\n",
    "spotify_population['acousticness'].hist(bins = np.arange(0,1.01,0.01))\n",
    "plt.show()\n",
    "\n",
    "# Update the histogram to use spotify_mysterious_sample\n",
    "spotify_mysterious_sample['acousticness'].hist(bins=np.arange(0, 1.01, 0.01))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa004c00",
   "metadata": {},
   "source": [
    "Are these findings generalizable?\n",
    "Let's look at another sample to see if it is representative of the population. This time, you'll look at the duration_minutes column of the Spotify dataset, which contains the length of the song in minutes.\n",
    "\n",
    "spotify_population and spotify_mysterious_sample2 are available; pandas, matplotlib.pyplot, and numpy are loaded using their standard aliases.\n",
    "\n",
    "Instructions\n",
    "\n",
    "Plot a histogram of duration_minutes from spotify_population with bins of width 0.5 from 0 to 15 using pandas .hist().\n",
    "Update the histogram code to use the spotify_mysterious_sample2 dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b6e553",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the distribution of duration_minutes as a histogram\n",
    "spotify_population['duration_minutes'].hist(bins = np.arange(0, 15.01, 0.5))\n",
    "plt.show()\n",
    "\n",
    "# Update the histogram to use spotify_mysterious_sample2\n",
    "spotify_mysterious_sample2['duration_minutes'].hist(bins=np.arange(0, 15.5, 0.5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd52281b",
   "metadata": {},
   "source": [
    "Generating random numbers\n",
    "You've used .sample() to generate pseudo-random numbers from a set of values in a DataFrame. A related task is to generate random numbers that follow a statistical distribution, like the uniform distribution or the normal distribution.\n",
    "\n",
    "Each random number generation function has distribution-specific arguments and an argument for specifying the number of random numbers to generate.\n",
    "\n",
    "matplotlib.pyplot is loaded as plt, and numpy is loaded as np.\n",
    "\n",
    "Instructions\n",
    "\n",
    "Generate 5000 numbers from a uniform distribution, setting the parameters low to -3 and high to 3.\n",
    "Generate 5000 numbers from a normal distribution, setting the parameters loc to 5 and scale to 2.\n",
    "Plot a histogram of uniforms with bins of width of 0.25 from -3 to 3 using plt.hist().\n",
    "Plot a histogram of normals with bins of width of 0.5 from -2 to 13 using plt.hist()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc77080",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate random numbers from a Uniform(-3, 3)\n",
    "uniforms = np.random.uniform(low = -3, high = 3, size = 5000)\n",
    "\n",
    "# Print uniforms\n",
    "print(uniforms)\n",
    "\n",
    "# Generate random numbers from a Normal(5, 2)\n",
    "normals = np.random.normal(loc = 5, scale = 2, size = 5000)\n",
    "\n",
    "# Print normals\n",
    "print(normals)\n",
    "\n",
    "# Plot a histogram of uniform values, binwidth 0.25\n",
    "plt.hist(uniforms, bins = np.arange(-3, 3.1, 0.25))\n",
    "plt.show()\n",
    "\n",
    "# Plot a histogram of normal values, binwidth 0.5\n",
    "plt.hist(normals, bins = np.arange(-2, 13.01, 0.5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2661f50",
   "metadata": {},
   "source": [
    "Simple random sampling\n",
    "The simplest method of sampling a population is the one you've seen already. It is known as simple random sampling (sometimes abbreviated to \"SRS\"), and involves picking rows at random, one at a time, where each row has the same chance of being picked as any other.\n",
    "\n",
    "In this chapter, you'll apply sampling methods to a synthetic (fictional) employee attrition dataset from IBM, where \"attrition\" in this context means leaving the company.\n",
    "\n",
    "attrition_pop is available; pandas as pd is loaded.\n",
    "\n",
    "Instructions\n",
    "\n",
    "Sample 70 rows from attrition_pop using simple random sampling, setting the random seed to 18900217.\n",
    "Print the sample dataset, attrition_samp. What do you notice about the indices?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92505f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample 70 rows using simple random sampling and set the seed\n",
    "attrition_samp = attrition_pop.sample(n= 70, random_state = 18900217)\n",
    "\n",
    "# Print the sample\n",
    "print(attrition_samp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe2e8ee9",
   "metadata": {},
   "source": [
    "Systematic sampling\n",
    "One sampling method that avoids randomness is called systematic sampling. Here, you pick rows from the population at regular intervals.\n",
    "\n",
    "For example, if the population dataset had one thousand rows, and you wanted a sample size of five, you could pick rows 0, 200, 400, 600, and 800.\n",
    "\n",
    "attrition_pop is available; pandas has been pre-loaded as pd.\n",
    "\n",
    "Instructions\n",
    "\n",
    "Set the sample size to 70.\n",
    "Calculate the population size from attrition_pop.\n",
    "Calculate the interval between the rows to be sampled.\n",
    "Systematically sample attrition_pop to get the rows of the population at each interval, starting at 0; assign the rows to attrition_sys_samp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c8505a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the sample size to 70\n",
    "sample_size = 70\n",
    "\n",
    "# Calculate the population size from attrition_pop\n",
    "pop_size = attrition_pop.shape[0]\n",
    "\n",
    "# Calculate the interval\n",
    "interval = pop_size // sample_size\n",
    "\n",
    "# Systematically sample 70 rows\n",
    "attrition_sys_samp = attrition_pop.iloc[::interval]\n",
    "\n",
    "# Print the sample\n",
    "print(attrition_sys_samp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a16d10",
   "metadata": {},
   "source": [
    "Is systematic sampling OK?\n",
    "Systematic sampling has a problem: if the data has been sorted, or there is some sort of pattern or meaning behind the row order, then the resulting sample may not be representative of the whole population. The problem can be solved by shuffling the rows, but then systematic sampling is equivalent to simple random sampling.\n",
    "\n",
    "Here you'll look at how to determine whether or not there is a problem.\n",
    "\n",
    "attrition_pop is available; pandas is loaded as pd, and matplotlib.pyplot as plt.\n",
    "\n",
    "Instructions\n",
    "\n",
    "Add an index column to attrition_pop, assigning the result to attrition_pop_id.\n",
    "Create a scatter plot of YearsAtCompany versus index for attrition_pop_id using pandas .plot()\n",
    "Randomly shuffle the rows of attrition_pop.\n",
    "Reset the row indexes, and add an index column to attrition_pop.\n",
    "Repeat the scatter plot of YearsAtCompany versus index, this time using attrition_shuffled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f40d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add an index column to attrition_pop\n",
    "attrition_pop_id = attrition_pop.reset_index()\n",
    "\n",
    "# Plot YearsAtCompany vs. index for attrition_pop_id\n",
    "attrition_pop_id.plot(x = 'index', y = 'YearsAtCompany', kind = 'scatter')\n",
    "plt.show()\n",
    "\n",
    "# Shuffle the rows of attrition_pop\n",
    "attrition_shuffled = attrition_pop.sample(frac=1)\n",
    "\n",
    "# Reset the row indexes and create an index column\n",
    "attrition_shuffled = attrition_shuffled.reset_index(drop = True).reset_index()\n",
    "\n",
    "# Plot YearsAtCompany vs. index for attrition_shuffled\n",
    "attrition_shuffled.plot(x = 'index', y = 'YearsAtCompany', kind = 'scatter')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f79321",
   "metadata": {},
   "source": [
    "Proportional stratified sampling\n",
    "If you are interested in subgroups within the population, then you may need to carefully control the counts of each subgroup within the population. Proportional stratified sampling results in subgroup sizes within the sample that are representative of the subgroup sizes within the population. It is equivalent to performing a simple random sample on each subgroup.\n",
    "\n",
    "attrition_pop is available; pandas is loaded with its usual alias.\n",
    "\n",
    "Instructions\n",
    "\n",
    "Get the proportion of employees by Education level from attrition_pop.\n",
    "Use proportional stratified sampling on attrition_pop to sample 40% of each Education group, setting the seed to 2022.\n",
    "Get the proportion of employees by Education level from attrition_strat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "936c1dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Proportion of employees by Education level\n",
    "education_counts_pop = attrition_pop['Education'].value_counts(normalize = True)\n",
    "\n",
    "# Print education_counts_pop\n",
    "print(education_counts_pop)\n",
    "\n",
    "# Proportional stratified sampling for 40% of each Education group\n",
    "attrition_strat = attrition_pop.groupby('Education').sample(frac = 0.4, random_state = 2022)\n",
    "\n",
    "# Print the sample\n",
    "print(attrition_strat)\n",
    "\n",
    "# Calculate the Education level proportions from attrition_strat\n",
    "education_counts_strat = attrition_strat['Education'].value_counts(normalize=True)\n",
    "\n",
    "# Print education_counts_strat\n",
    "print(education_counts_strat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fbd13ce",
   "metadata": {},
   "source": [
    "Equal counts stratified sampling\n",
    "If one subgroup is larger than another subgroup in the population, but you don't want to reflect that difference in your analysis, then you can use equal counts stratified sampling to generate samples where each subgroup has the same amount of data. For example, if you are analyzing blood types, O is the most common blood type worldwide, but you may wish to have equal amounts of O, A, B, and AB in your sample.\n",
    "\n",
    "attrition_pop is available; pandas is loaded with its usual alias.\n",
    "\n",
    "Instructions\n",
    "\n",
    "Use equal counts stratified sampling on attrition_pop to get 30 employees from each Education group, setting the seed to 2022.\n",
    "Get the proportion of employees by Education level from attrition_eq."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ba09c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get 30 employees from each Education group\n",
    "attrition_eq = attrition_pop.groupby('Education').sample(n=30, random_state=2022)\n",
    "\n",
    "# Print the sample\n",
    "print(attrition_eq)\n",
    "\n",
    "# Get the proportions from attrition_eq\n",
    "education_counts_eq = attrition_eq['Education'].value_counts(normalize=True)\n",
    "\n",
    "# Print the results\n",
    "print(education_counts_eq)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a39cbc",
   "metadata": {},
   "source": [
    "Weighted sampling\n",
    "Stratified sampling provides rules about the probability of picking rows from your dataset at the subgroup level. A generalization of this is weighted sampling, which lets you specify rules about the probability of picking rows at the row level. The probability of picking any given row is proportional to the weight value for that row.\n",
    "\n",
    "attrition_pop is available; pandas, matplotlib.pyplot, and numpy are loaded with their usual aliases.\n",
    "\n",
    "Instructions\n",
    "\n",
    "Plot YearsAtCompany from attrition_pop as a histogram with bins of width 1 from 0 to 40.\n",
    "Sample 400 employees from attrition_pop weighted by YearsAtCompany.\n",
    "Plot YearsAtCompany from attrition_weight as a histogram with bins of width 1 from 0 to 40."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fdecb4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot YearsAtCompany from attrition_pop as a histogram\n",
    "attrition_pop['YearsAtCompany'].hist(bins=np.arange(0, 41, 1))\n",
    "plt.show()\n",
    "\n",
    "# Sample 400 employees weighted by YearsAtCompany\n",
    "attrition_weight = attrition_pop.sample(n=400, weights='YearsAtCompany')\n",
    "\n",
    "# Print the sample\n",
    "print(attrition_weight)\n",
    "\n",
    "# Plot YearsAtCompany from attrition_weight as a histogram\n",
    "attrition_weight['YearsAtCompany'].hist(bins=np.arange(0, 41, 1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde90744",
   "metadata": {},
   "source": [
    "Cluster sampling\n",
    "Now that you know when to use cluster sampling, it's time to put it into action. In this exercise, you'll explore the JobRole column of the attrition dataset. You can think of each job role as a subgroup of the whole population of employees.\n",
    "\n",
    "attrition_pop is available; pandas is loaded with its usual alias, and the random package is available. A seed of 19790801 has also been set with random.seed().\n",
    "\n",
    "Instructions\n",
    "\n",
    "Create a list of unique JobRole values from attrition_pop, and assign to job_roles_pop.\n",
    "Randomly sample four JobRole values from job_roles_pop.\n",
    "Subset attrition_pop for the sampled job roles by filtering for rows where JobRole is in job_roles_samp.\n",
    "Remove any unused categories from JobRole.\n",
    "For each job role in the filtered dataset, take a random sample of ten rows, setting the seed to 2022."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cfa36e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of unique JobRole values\n",
    "job_roles_pop = list(attrition_pop['JobRole'].unique())\n",
    "\n",
    "# Randomly sample four JobRole values\n",
    "job_roles_samp = random.sample(job_roles_pop, k=4)\n",
    "\n",
    "# Print the result\n",
    "print(job_roles_samp)\n",
    "\n",
    "# Filter for rows where JobRole is in job_roles_samp\n",
    "jobrole_condition = attrition_pop['JobRole'].isin(job_roles_samp)\n",
    "attrition_filtered = attrition_pop[jobrole_condition]\n",
    "\n",
    "# Print the result\n",
    "print(attrition_filtered)\n",
    "\n",
    "# Remove categories with no rows\n",
    "attrition_filtered['JobRole'] = attrition_filtered['JobRole'].cat.remove_unused_categories()\n",
    "\n",
    "# Randomly sample 10 employees from each sampled job role\n",
    "attrition_clust = attrition_filtered.groupby('JobRole').sample(n=10, random_state = 2022)\n",
    "\n",
    "# Print the sample\n",
    "print(attrition_clust)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a79f59",
   "metadata": {},
   "source": [
    "3 kinds of sampling\n",
    "You're going to compare the performance of point estimates using simple, stratified, and cluster sampling. Before doing that, you'll have to set up the samples.\n",
    "\n",
    "You'll use the RelationshipSatisfaction column of the attrition_pop dataset, which categorizes the employee's relationship with the company. It has four levels: Low, Medium, High, and Very_High. pandas has been loaded with its usual alias, and the random package has been loaded.\n",
    "\n",
    "Instructions\n",
    "\n",
    "Perform simple random sampling on attrition_pop to get one-quarter of the population, setting the seed to 2022.\n",
    "Perform stratified sampling on attrition_pop to sample one-quarter of each RelationshipSatisfaction group, setting the seed to 2022.\n",
    "Create a list of unique values from attrition_pop's RelationshipSatisfaction column.\n",
    "Randomly sample satisfaction_unique to get two values.\n",
    "Subset the population for rows where RelationshipSatisfaction is in satisfaction_samp and clear any unused categories from RelationshipSatisfaction; assign to attrition_clust_prep.\n",
    "Perform cluster sampling on the selected satisfaction groups, sampling one quarter of the population and setting the seed to 2022."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6444b706",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform simple random sampling to get 0.25 of the population\n",
    "attrition_srs = attrition_pop.sample(frac=0.25, random_state=2022)\n",
    "\n",
    "# Perform stratified sampling to get 0.25 of each relationship group\n",
    "attrition_strat = attrition_pop.groupby('RelationshipSatisfaction').sample(frac=0.25, random_state=2022)\n",
    "\n",
    "# Create a list of unique RelationshipSatisfaction values\n",
    "satisfaction_unique = list(attrition_pop['RelationshipSatisfaction'].unique())\n",
    "\n",
    "# Randomly sample 2 unique satisfaction values\n",
    "satisfaction_samp = random.sample(satisfaction_unique, k=2)\n",
    "\n",
    "# Filter for satisfaction_samp and clear unused categories from RelationshipSatisfaction\n",
    "satis_condition = attrition_pop['RelationshipSatisfaction'].isin(satisfaction_samp)\n",
    "attrition_clust_prep = attrition_pop[satis_condition]\n",
    "attrition_clust_prep['RelationshipSatisfaction'] = attrition_clust_prep['RelationshipSatisfaction'].cat.remove_unused_categories()\n",
    "\n",
    "# Perform cluster sampling on the selected group, getting 0.25 of attrition_pop\n",
    "attrition_clust = attrition_clust_prep.groupby(\"RelationshipSatisfaction\")\\\n",
    "    .sample(n=len(attrition_pop) // 4, random_state=2022)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936d44a7",
   "metadata": {},
   "source": [
    "Comparing point estimates\n",
    "Now that you have three types of sample (simple, stratified, and cluster), you can compare point estimates from each sample to the population parameter. That is, you can calculate the same summary statistic on each sample and see how it compares to the summary statistic for the population.\n",
    "\n",
    "Here, we'll look at how satisfaction with the company affects whether or not the employee leaves the company. That is, you'll calculate the proportion of employees who left the company (they have an Attrition value of 1) for each value of RelationshipSatisfaction.\n",
    "\n",
    "attrition_pop, attrition_srs, attrition_strat, and attrition_clust are available; pandas is loaded with its usual alias.\n",
    "\n",
    "Instructions\n",
    "\n",
    "Group attrition_pop by RelationshipSatisfaction levels and calculate the mean of Attrition for each level.\n",
    "Calculate the proportion of employee attrition for each relationship satisfaction group, this time on the simple random sample, attrition_srs.\n",
    "Calculate the proportion of employee attrition for each relationship satisfaction group, this time on the stratified sample, attrition_strat.\n",
    "Calculate the proportion of employee attrition for each relationship satisfaction group, this time on the cluster sample, attrition_clust."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58386723",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean Attrition by RelationshipSatisfaction group\n",
    "mean_attrition_pop = attrition_pop.groupby('RelationshipSatisfaction')['Attrition'].mean()\n",
    "\n",
    "# Print the result\n",
    "print(mean_attrition_pop)\n",
    "\n",
    "# Calculate the same thing for the simple random sample \n",
    "mean_attrition_srs = attrition_srs.groupby('RelationshipSatisfaction')['Attrition'].mean()\n",
    "\n",
    "# Print the result\n",
    "print(mean_attrition_srs)\n",
    "\n",
    "# Calculate the same thing for the stratified sample \n",
    "mean_attrition_strat = attrition_strat.groupby('RelationshipSatisfaction')['Attrition'].mean()\n",
    "\n",
    "# Print the result\n",
    "print(mean_attrition_strat)\n",
    "\n",
    "# Calculate the same thing for the cluster sample \n",
    "mean_attrition_clust = attrition_clust.groupby('RelationshipSatisfaction')['Attrition'].mean()\n",
    "\n",
    "# Print the result\n",
    "print(mean_attrition_clust)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "317ab26d",
   "metadata": {},
   "source": [
    "Calculating relative errors\n",
    "The size of the sample you take affects how accurately the point estimates reflect the corresponding population parameter. For example, when you calculate a sample mean, you want it to be close to the population mean. However, if your sample is too small, this might not be the case.\n",
    "\n",
    "The most common metric for assessing accuracy is relative error. This is the absolute difference between the population parameter and the point estimate, all divided by the population parameter. It is sometimes expressed as a percentage.\n",
    "\n",
    "attrition_pop and mean_attrition_pop (the mean of the Attrition column of attrition_pop) are available; pandas is loaded as pd.\n",
    "\n",
    "Instructions\n",
    "\n",
    "Generate a simple random sample from attrition_pop of fifty rows, setting the seed to 2022.\n",
    "Calculate the mean employee Attrition in the sample.\n",
    "Calculate the relative error between mean_attrition_srs50 and mean_attrition_pop as a percentage.\n",
    "Calculate the relative error percentage again. This time, use a simple random sample of one hundred rows of attrition_pop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8181a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a simple random sample of 50 rows, with seed 2022\n",
    "attrition_srs50 = attrition_pop.sample(n=50, random_state=2022)\n",
    "\n",
    "# Calculate the mean employee attrition in the sample\n",
    "mean_attrition_srs50 = attrition_srs50['Attrition'].mean()\n",
    "\n",
    "# Calculate the relative error percentage\n",
    "rel_error_pct50 = 100 * abs(mean_attrition_pop - mean_attrition_srs50) / mean_attrition_pop\n",
    "\n",
    "# Print rel_error_pct50\n",
    "print(rel_error_pct50)\n",
    "\n",
    "# Generate a simple random sample of 100 rows, with seed 2022\n",
    "attrition_srs100 =  attrition_pop.sample(n=100, random_state=2022)\n",
    "\n",
    "# Calculate the mean employee attrition in the sample\n",
    "mean_attrition_srs100 = attrition_srs100['Attrition'].mean()\n",
    "\n",
    "# Calculate the relative error percentage\n",
    "rel_error_pct100 = 100 * abs(mean_attrition_pop - mean_attrition_srs100)/mean_attrition_pop\n",
    "\n",
    "# Print rel_error_pct100\n",
    "print(rel_error_pct100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ab3086",
   "metadata": {},
   "source": [
    "Replicating samples\n",
    "When you calculate a point estimate such as a sample mean, the value you calculate depends on the rows that were included in the sample. That means that there is some randomness in the answer. In order to quantify the variation caused by this randomness, you can create many samples and calculate the sample mean (or another statistic) for each sample.\n",
    "\n",
    "attrition_pop is available; pandas and matplotlib.pyplot are loaded with their usual aliases.\n",
    "\n",
    "Instructions\n",
    "\n",
    "Replicate the provided code so that it runs 500 times. Assign the resulting list of sample means to mean_attritions.\n",
    "Draw a histogram of the mean_attritions list with 16 bins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa3c3d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty list\n",
    "mean_attritions = []\n",
    "# Loop 500 times to create 500 sample means\n",
    "for i in range(500):\n",
    "\tmean_attritions.append(\n",
    "    \tattrition_pop.sample(n=60)['Attrition'].mean()\n",
    "\t)\n",
    "  \n",
    "# Print out the first few entries of the list\n",
    "print(mean_attritions[0:5])\n",
    "\n",
    "# Create a histogram of the 500 sample means\n",
    "plt.hist(mean_attritions ,bins = 16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b0018b5",
   "metadata": {},
   "source": [
    "Exact sampling distribution\n",
    "To quantify how the point estimate (sample statistic) you are interested in varies, you need to know all the possible values it can take and how often. That is, you need to know its distribution.\n",
    "\n",
    "The distribution of a sample statistic is called the sampling distribution. When we can calculate this exactly, rather than using an approximation, it is known as the exact sampling distribution.\n",
    "\n",
    "Let's take another look at the sampling distribution of dice rolls. This time, we'll look at five eight-sided dice. (These have the numbers one to eight.)\n",
    "\n",
    "pandas, numpy, and matplotlib.pyplot are loaded with their usual aliases. The expand_grid() function is also available, which expects a dictionary of key-value pairs as its argument. The definition of the expand_grid() function is provided in the pandas documentation.\n",
    "\n",
    "Instructions\n",
    "\n",
    "Expand a grid representing 5 8-sided dice. That is, create a DataFrame with five columns from a dictionary, named die1 to die5. The rows should contain all possibilities for throwing five dice, each numbered 1 to 8.\n",
    "Add a column, mean_roll, to dice, that contains the mean of the five rolls as a categorical.\n",
    "Create a bar plot of the mean_roll categorical column, so it displays the count of each mean_roll in increasing order from 1.0 to 8.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d355311c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expand a grid representing 5 8-sided dice\n",
    "dice = expand_grid(\n",
    "    {\n",
    "        'die1':[1, 2, 3, 4, 5, 6, 7, 8],\n",
    "        'die2':[1, 2, 3, 4, 5, 6, 7, 8],\n",
    "        'die3':[1, 2, 3, 4, 5, 6, 7, 8],\n",
    "        'die4':[1, 2, 3, 4, 5, 6, 7, 8],\n",
    "        'die5':[1, 2, 3, 4, 5, 6, 7, 8]\n",
    "    }\n",
    ")\n",
    "\n",
    "# Add a column of mean rolls and convert to a categorical\n",
    "dice['mean_roll'] = (dice['die1']+dice['die2']+dice['die3']+dice['die4']+dice['die5'])/5 \n",
    "                     \n",
    "                    \n",
    "dice['mean_roll'] = dice['mean_roll'].astype('category')\n",
    "\n",
    "# Print the result\n",
    "print(dice)\n",
    "\n",
    "# Draw a bar plot of mean_roll\n",
    "dice['mean_roll'].value_counts(sort=False).plot(kind = 'bar')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6acf72f6",
   "metadata": {},
   "source": [
    "Approximate sampling distribution\n",
    "Calculating the exact sampling distribution is only possible in very simple situations. With just five eight-sided dice, the number of possible rolls is 8**5, which is over thirty thousand. When the dataset is more complicated, for example, where a variable has hundreds or thousands of categories, the number of possible outcomes becomes too difficult to compute exactly.\n",
    "\n",
    "In this situation, you can calculate an approximate sampling distribution by simulating the exact sampling distribution. That is, you can repeat a procedure over and over again to simulate both the sampling process and the sample statistic calculation process.\n",
    "\n",
    "pandas, numpy, and matplotlib.pyplot are loaded with their usual aliases.\n",
    "\n",
    "Instructions\n",
    "\n",
    "Sample one to eight, five times, with replacement. Assign to five_rolls.\n",
    "Calculate the mean of five_rolls.\n",
    "Replicate the sampling code 1000 times, assigning each result to the list sample_means_1000.\n",
    "Plot sample_means_1000 as a histogram with 20 bins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14edaa19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample one to eight, five times, with replacement\n",
    "five_rolls = np.random.choice(list(range(1,9)), size = 5, replace = True)\n",
    "\n",
    "# Print the mean of five_rolls\n",
    "print(five_rolls.mean())\n",
    "\n",
    "# Replicate the sampling code 1000 times\n",
    "sample_means_1000 = []\n",
    "for i in range(1000):\n",
    "    sample_means_1000.append(\n",
    "  \t\tnp.random.choice(list(range(1, 9)), size=5, replace=True).mean()\n",
    "    )\n",
    "    \n",
    "# Print the first 10 entries of the result\n",
    "print(sample_means_1000[0:10])\n",
    "\n",
    "# Draw a histogram of sample_means_1000 with 20 bins\n",
    "plt.hist(sample_means_1000, bins = 20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e53dddd",
   "metadata": {},
   "source": [
    "Population & sampling distribution means\n",
    "One of the useful features of sampling distributions is that you can quantify them. Specifically, you can calculate summary statistics on them. Here, you'll look at the relationship between the mean of the sampling distribution and the population parameter's mean.\n",
    "\n",
    "Three sampling distributions are provided. For each, the employee attrition dataset was sampled using simple random sampling, then the mean attrition was calculated. This was done 1000 times to get a sampling distribution of mean attritions. One sampling distribution used a sample size of 5 for each replicate, one used 50, and one used 500.\n",
    "\n",
    "attrition_pop, sampling_distribution_5, sampling_distribution_50, and sampling_distribution_500 are available; numpy as np is loaded.\n",
    "\n",
    "Instructions\n",
    "\n",
    "Calculate the mean of sampling_distribution_5, sampling_distribution_50, and sampling_distribution_500 (a mean of sample means)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ea74b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the mean of the mean attritions for each sampling distribution\n",
    "mean_of_means_5 = np.mean(sampling_distribution_5)\n",
    "mean_of_means_50 = np.mean(sampling_distribution_50)\n",
    "mean_of_means_500 = np.mean(sampling_distribution_500)\n",
    "\n",
    "# Print the results\n",
    "print(mean_of_means_5)\n",
    "print(mean_of_means_50)\n",
    "print(mean_of_means_500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6541c7aa",
   "metadata": {},
   "source": [
    "Population & sampling distribution variation\n",
    "You just calculated the mean of the sampling distribution and saw how it is an estimate of the corresponding population parameter. Similarly, as a result of the central limit theorem, the standard deviation of the sampling distribution has an interesting relationship with the population parameter's standard deviation and the sample size.\n",
    "\n",
    "attrition_pop, sampling_distribution_5, sampling_distribution_50, and sampling_distribution_500 are available; numpy is loaded with its usual alias.\n",
    "\n",
    "Instructions\n",
    "\n",
    "Calculate the standard deviation of sampling_distribution_5, sampling_distribution_50, and sampling_distribution_500 (a standard deviation of sample means)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c4c95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the std. dev. of the mean attritions for each sampling distribution\n",
    "sd_of_means_5 = np.std(sampling_distribution_5, ddof = 1)\n",
    "sd_of_means_50 = np.std(sampling_distribution_50, ddof = 1)\n",
    "sd_of_means_500 = np.std(sampling_distribution_500, ddof = 1)\n",
    "\n",
    "# Print the results\n",
    "print(sd_of_means_5)\n",
    "print(sd_of_means_50)\n",
    "print(sd_of_means_500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca2b028",
   "metadata": {},
   "source": [
    "Generating a bootstrap distribution\n",
    "The process for generating a bootstrap distribution is similar to the process for generating a sampling distribution; only the first step is different.\n",
    "\n",
    "To make a sampling distribution, you start with the population and sample without replacement. To make a bootstrap distribution, you start with a sample and sample that with replacement. After that, the steps are the same: calculate the summary statistic that you are interested in on that sample/resample, then replicate the process many times. In each case, you can visualize the distribution with a histogram.\n",
    "\n",
    "Here, spotify_sample is a subset of the spotify_population dataset. To make it easier to see how resampling works, a row index column called 'index' has been added, and only the artist name, song name, and danceability columns have been included.\n",
    "\n",
    "spotify_sample is available; pandas, numpy, and matplotlib.pyplot are loaded with their usual aliases.\n",
    "\n",
    "Instructions\n",
    "\n",
    "Generate a single bootstrap resample from spotify_sample.\n",
    "Calculate the mean of the danceability column of spotify_1_resample using numpy.\n",
    "Replicate the expression provided 1000 times.\n",
    "Create a bootstrap distribution by drawing a histogram of mean_danceability_1000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ded004",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate 1 bootstrap resample\n",
    "spotify_1_resample = spotify_sample.sample(frac = 1, replace = True)\n",
    "\n",
    "# Print the resample\n",
    "print(spotify_1_resample)\n",
    "\n",
    "# Calculate mean danceability of resample\n",
    "mean_danceability_1 = np.mean(spotify_1_resample['danceability'])\n",
    "\n",
    "# Print the result\n",
    "print(mean_danceability_1)\n",
    "\n",
    "# Replicate this 1000 times\n",
    "mean_danceability_1000 = []\n",
    "for i in range(1000):\n",
    "\tmean_danceability_1000.append(\n",
    "        np.mean(spotify_sample.sample(frac=1, replace=True)['danceability'])\n",
    "\t)\n",
    "  \n",
    "# Print the result\n",
    "print(mean_danceability_1000)\n",
    "\n",
    "# Draw a histogram of the resample means\n",
    "plt.hist(mean_danceability_1000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c5a59a",
   "metadata": {},
   "source": [
    "Sampling distribution vs. bootstrap distribution\n",
    "The sampling distribution and bootstrap distribution are closely linked. In situations where you can repeatedly sample from a population (these occasions are rare), it's helpful to generate both the sampling distribution and the bootstrap distribution, one after the other, to see how they are related.\n",
    "\n",
    "Here, the statistic you are interested in is the mean popularity score of the songs.\n",
    "\n",
    "spotify_population (the whole dataset) and spotify_sample (500 randomly sampled rows from spotify_population) are available; pandas and numpy are loaded with their usual aliases.\n",
    "\n",
    "Instructions\n",
    "\n",
    "Generate a sampling distribution of 2000 replicates.\n",
    "Sample 500 rows of the population without replacement and calculate the mean popularity.\n",
    "Generate a bootstrap distribution of 2000 replicates.\n",
    "Sample 500 rows of the sample with replacement and calculate the mean popularity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c036aa44",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_popularity_2000_samp = []\n",
    "\n",
    "# Generate a sampling distribution of 2000 replicates\n",
    "for i in range(2000):\n",
    "    mean_popularity_2000_samp.append(\n",
    "    \t# Sample 500 rows and calculate the mean popularity \n",
    "    \tnp.mean(spotify_population.sample(n = 500, replace = False)['popularity'])\n",
    "    )\n",
    "\n",
    "# Print the sampling distribution results\n",
    "print(mean_popularity_2000_samp)\n",
    "\n",
    "mean_popularity_2000_boot = []\n",
    "\n",
    "# Generate a bootstrap distribution of 2000 replicates\n",
    "for i in range(2000):\n",
    "    mean_popularity_2000_boot.append(\n",
    "    \t# Resample 500 rows and calculate the mean popularity     \n",
    "    \tnp.mean(spotify_sample.sample(frac = 1, replace = True)['popularity'])\n",
    "    )\n",
    "\n",
    "# Print the bootstrap distribution results\n",
    "print(mean_popularity_2000_boot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4555ed3a",
   "metadata": {},
   "source": [
    "Compare sampling and bootstrap means\n",
    "To make calculation easier, distributions similar to those calculated from the previous exercise have been included, this time using a sample size of 5000.\n",
    "\n",
    "spotify_population, spotify_sample, sampling_distribution, and bootstrap_distribution are available; pandas and numpy are loaded with their usual aliases.\n",
    "\n",
    "Instructions\n",
    "\n",
    "Calculate the mean popularity in 4 ways:\n",
    "Population: from spotify_population, take the mean of popularity.\n",
    "Sample: from spotify_sample, take the mean of popularity.\n",
    "Sampling distribution: from sampling_distribution, take its mean.\n",
    "Bootstrap distribution: from bootstrap_distribution, take its mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77349a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the population mean popularity\n",
    "pop_mean = spotify_population['popularity'].mean()\n",
    "\n",
    "# Calculate the original sample mean popularity\n",
    "samp_mean = np.mean(spotify_sample.sample(frac = 1, replace = False)['popularity'])\n",
    "\n",
    "# Calculate the sampling dist'n estimate of mean popularity\n",
    "samp_distn_mean = np.mean(sampling_distribution)\n",
    "\n",
    "# Calculate the bootstrap dist'n estimate of mean popularity\n",
    "boot_distn_mean = np.mean(bootstrap_distribution)\n",
    "\n",
    "# Print the means\n",
    "print([pop_mean, samp_mean, samp_distn_mean, boot_distn_mean])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "727d386d",
   "metadata": {},
   "source": [
    "Compare sampling and bootstrap standard deviations\n",
    "In the same way that you looked at how the sampling distribution and bootstrap distribution could be used to estimate the population mean, you'll now take a look at how they can be used to estimate variation, or more specifically, the standard deviation, in the population.\n",
    "\n",
    "Recall that the sample size is 5000.\n",
    "\n",
    "spotify_population, spotify_sample, sampling_distribution, and bootstrap_distribution are available; pandas and numpy are loaded with their usual aliases.\n",
    "\n",
    "Instructions\n",
    "\n",
    "Calculate the standard deviation of popularity in 4 ways.\n",
    "Population: from spotify_population, take the standard deviation of popularity.\n",
    "Original sample: from spotify_sample, take the standard deviation of popularity.\n",
    "Sampling distribution: from sampling_distribution, take its standard deviation and multiply by the square root of the sample size (5000).\n",
    "Bootstrap distribution: from bootstrap_distribution, take its standard deviation and multiply by the square root of the sample size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169fb502",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the population std dev popularity\n",
    "pop_sd = spotify_population['popularity'].std(ddof = 0)\n",
    "\n",
    "# Calculate the original sample std dev popularity\n",
    "samp_sd = spotify_sample['popularity'].std()\n",
    "\n",
    "# Calculate the sampling dist'n estimate of std dev popularity\n",
    "samp_distn_sd = np.std(sampling_distribution, ddof = 1)*np.sqrt(5000)\n",
    "\n",
    "# Calculate the bootstrap dist'n estimate of std dev popularity\n",
    "boot_distn_sd = np.std(bootstrap_distribution, ddof = 1)*np.sqrt(5000)\n",
    "\n",
    "# Print the standard deviations\n",
    "print([pop_sd, samp_sd, samp_distn_sd, boot_distn_sd])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "278ed0b1",
   "metadata": {},
   "source": [
    "Calculating confidence intervals\n",
    "You have learned about two methods for calculating confidence intervals: the quantile method and the standard error method. The standard error method involves using the inverse cumulative distribution function (inverse CDF) of the normal distribution to calculate confidence intervals. In this exercise, you'll perform these two methods on the Spotify data.\n",
    "\n",
    "spotify_population, spotify_sample, and bootstrap_distribution are available; pandas and numpy are loaded with their usual aliases, and norm has been loaded from scipy.stats.\n",
    "\n",
    "Instructions\n",
    "\n",
    "Generate a 95% confidence interval using the quantile method on the bootstrap distribution, setting the 0.025 quantile as lower_quant and the 0.975 quantile as upper_quant.\n",
    "Generate a 95% confidence interval using the standard error method from the bootstrap distribution.\n",
    "\n",
    "Calculate point_estimate as the mean of bootstrap_distribution, and standard_error as the standard deviation of bootstrap_distribution.\n",
    "Calculate lower_se as the 0.025 quantile of an inv. CDF from a normal distribution with mean point_estimate and standard deviation standard_error.\n",
    "Calculate upper_se as the 0.975 quantile of that same inv. CDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333a70cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a 95% confidence interval using the quantile method\n",
    "lower_quant = np.quantile(bootstrap_distribution, 0.025)\n",
    "upper_quant = np.quantile(bootstrap_distribution, 0.975)\n",
    "\n",
    "# Print quantile method confidence interval\n",
    "print((lower_quant, upper_quant))\n",
    "\n",
    "# Find the mean and std dev of the bootstrap distribution\n",
    "point_estimate = np.mean(bootstrap_distribution)\n",
    "standard_error = np.std(bootstrap_distribution, ddof = 1)\n",
    "\n",
    "# Find the lower limit of the confidence interval\n",
    "lower_se = norm.ppf(0.025, loc = point_estimate, scale = standard_error)\n",
    "\n",
    "# Find the upper limit of the confidence interval\n",
    "upper_se = norm.ppf(0.975, loc = point_estimate, scale = standard_error)\n",
    "\n",
    "# Print standard error method confidence interval\n",
    "print((lower_se, upper_se))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
